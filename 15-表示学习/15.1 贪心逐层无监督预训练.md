无监督学习在深度神经网络的复兴上起到了关键的、历史性的作用，它使研究 者首次可以训练不含诸如卷积或者循环这类特殊结构的深度监督网络。我们将这一 过程称为无监督预训练(unsupervised pretraining)，或者更精确地，贪心逐层无监 督预训练(greedy layer-wise unsupervised pretraining)。此过程是一个任务(无监 督学习，尝试获取输入分布的形状)的表示如何有助于另一个任务(具有相同输入 域的监督学习)的典型示例。

贪心逐层无监督预训练依赖于单层表示学习算法，例如 RBM、单层自编码器、稀疏编码模型或其他学习潜在表示的模型。每一层使用无监督学习预训练，将前一 层的输出作为输入，输出数据的新的表示