随机梯度下降 (stochastic gradient descent, SGD)

随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。具 体而言，在算法的每一步，我们从训练集中均匀抽出一 小批量(minibatch)样本

* 小批量的数目 m′ 通常是一个相对较小的数，从一到几百。重 要的是，当训练集大小 m 增长时，m′ 通常是固定的

