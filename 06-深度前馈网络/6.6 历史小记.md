前馈网络可以被视为一种高效的非线性函数近似器，它以使用梯度下降来最小 化函数近似误差为基础。从这个角度来看，现代前馈网络是一般函数近似任务的几 个世纪进步的结晶。



其中一个算法上的变化是用交叉熵族损失函数替代均方误差损失函数。均方误 差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原 理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具有 sigmoid 和 softmax 输出的模型的性能，而当使用均方误差损失时会存在饱和和
学习缓慢的问题。

另一个显著改善前馈网络性能的算法上的主要变化是使用分段线性隐藏单元来 替代 sigmoid 隐藏单元，例如用整流线性单元