传统的神经网络对一些输入变量 􏰈 施加确定性变换。当开发生成模型时，我们 经常希望扩展神经网络以实现 􏰈 的随机变换。这样做的一个直接方法是使用额外输 入 􏳎(从一些简单的概率分布采样得到，如均匀或高斯分布)来增强神经网络。神经 网络在内部仍可以继续执行确定性计算，但是函数 f (􏰈, 􏳎) 对于不能访问 􏳎 的观察者 来说将是随机的。假设 f 是连续可微的，我们可以像往常一样使用反向传播计算训 练所需的梯度。