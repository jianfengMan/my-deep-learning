自然语言处理(Natural Language Processing)

语言模型(language model)定义了自然语言中标记序列的概率分布。根据模型 的设计，标记可以是词、字符、甚至是字节。标记总是离散的实体。

n-gram 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 Pn 很可能为零(即使元组 (xt−n+1, . . . , xt) 可能出现在测试集中,大多数 n-gram 模型采用某种形式的平滑(smoothing)。 平滑技术将概率质量从观察到的元组转移到类似的未观察到的元组

神经语言模型(Neural Language Model, NLM)是一类用来克服维数灾难的语 言模型，它使用词的分布式表示对自然语言序列建模,不同于 基于类的 n-gram 模型，神经语言模型在能够识别两个相似的词，并且不丧失将每个 词编码为彼此不同的能力



词袋(bag of words)。词袋具有稀疏向量 􏲥，其中 vi 表示 词汇表中的词 i 存不存在文档中。或者，vi 可以指示词 i 出现的次数

n-gram 模型相对神经网络的主要优点是 n-gram 模型具有更高的模型容量(通 过存储非常多的元组的频率)，并且处理样本只需非常少的计算量(通过查找只匹配当前上下文的几个元组)。如果我们使用哈希表或树来访问计数，那么用于 n-gram 的 计算量几乎与容量无关

相比之下，将神经网络的参数数目加倍通常也大致加倍计 算时间。当然，避免每次计算时使用所有参数的模型是一个例外。嵌入层每次只索 引单个嵌入，所以我们可以增加词汇量，而不会增加每个样本的计算时间。一些其 他模型，例如平铺卷积网络，可以在减少参数共享程度的同时添加参数以保持相同 的计算量。然而，基于矩阵乘法的典型神经网络层需要与参数数量成比例的计算量

因此，增加容量的一种简单方法是将两种方法结合，由神经语言模型和 n- gram 语言模型组成集成  